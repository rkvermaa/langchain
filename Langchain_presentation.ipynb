{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUcv+GGjd3y4ZbnVscq58z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rkvermaa/langchain/blob/main/Langchain_presentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quick glimpse of LangChain**"
      ],
      "metadata": {
        "id": "8c_OoPZiUST8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8k02ntVUiUV",
        "outputId": "ffdea251-0788-4aab-8d86-8540f1e119cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Environment setup**\n",
        "\n",
        "Installing langchain itself, is not enough. Using LangChain will usually require integrations with one or more model providers, data stores, apis, etc.\n",
        "Let's use OpenAI for this demo"
      ],
      "metadata": {
        "id": "LmM_cw7yUrDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF-PYK9jVBxc",
        "outputId": "4ba786c3-aca5-4c08-f383-fea3df300a84"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/76.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m71.7/76.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set environment variables\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "vF7ZZtvwVM4u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building Large Language Models (LLMs)**\n",
        "\n",
        "1. Now that we have installed LangChain and set up our environment, we can start\n",
        "building our language model application.\n",
        "\n",
        "2. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\n",
        "\n",
        "3. In order to do this, we first need to import the LLM wrapper."
      ],
      "metadata": {
        "id": "34B7RS6FVU3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### For this demo, lets just get predictions\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "vdFAMkcGVyVQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the wrapper\n",
        "llm = OpenAI()"
      ],
      "metadata": {
        "id": "2HMe3N-OV05r"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, we can call it on some input\n",
        "text = \"What would be a good company name for a company that makes colorful shirts?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw0PGO87V3jv",
        "outputId": "e7bd0341-136d-4494-b986-5d72eccb927d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Crazy Colorful Tees\n"
          ]
        }
      ]
    }
  ]
}